"""
Memory-Optimized GPU CFR Training with Advanced Memory Management
================================================================================

This script implements maximum memory utilization for GPU-accelerated CFR training
with pre-allocation, memory pooling, and adaptive batch sizing for optimal performance.
"""

import sys
import os
import logging
import numpy as np
import time
import json
from datetime import datetime

# Set up aggressive logging for memory monitoring
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [Memory] %(message)s',
    handlers=[
        logging.FileHandler('memory_optimized_training.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

# GPU memory management
try:
    import cupy as cp
    import cupy.cuda.memory
    GPU_AVAILABLE = True
    logger.info("‚úÖ CuPy detected - GPU acceleration enabled")
    
    # Get GPU memory info
    mempool = cp.get_default_memory_pool()
    pinned_mempool = cp.get_default_pinned_memory_pool()
    
    # Get GPU properties
    device = cp.cuda.Device()
    gpu_memory_info = cp.cuda.runtime.memGetInfo()
    free_memory = gpu_memory_info[0]
    total_memory = gpu_memory_info[1] 
    
    logger.info(f"üîß GPU Memory: {free_memory / 1024**3:.1f}GB free / {total_memory / 1024**3:.1f}GB total")
    
except ImportError:
    GPU_AVAILABLE = False
    logger.error("‚ùå CuPy not available - cannot run memory optimization")
    sys.exit(1)

class MemoryOptimizedGPUTrainer:
    """CFR Trainer with maximum memory utilization and pre-allocation."""
    
    def __init__(self, num_players=6, memory_fraction=0.85):
        """
        Initialize with maximum memory allocation.
        
        Args:
            num_players: Number of players in the game
            memory_fraction: Fraction of GPU memory to use (0.85 = 85%)
        """
        self.num_players = num_players
        self.memory_fraction = memory_fraction
        
        # Import required modules
        sys.path.append(os.path.dirname(os.path.abspath(__file__)))
        from gpu_accelerated_equity import GPUEquityCalculator
        from hand_evaluator import HandEvaluator
        
        # Initialize core components
        self.hand_evaluator = HandEvaluator()
        self.gpu_equity_calculator = GPUEquityCalculator(use_gpu=True)
        
        # Calculate optimal memory allocation
        self._calculate_memory_layout()
        self._pre_allocate_gpu_memory()
        
        # Statistics
        self.training_stats = {
            'start_time': None,
            'iterations_completed': 0,
            'total_simulations': 0,
            'memory_peak_usage': 0,
            'batch_times': [],
            'throughput_history': []
        }
        
    def _calculate_memory_layout(self):
        """Calculate optimal batch sizes and memory allocation."""
        
        # Get available GPU memory
        gpu_memory_info = cp.cuda.runtime.memGetInfo()
        available_memory = gpu_memory_info[0] * self.memory_fraction
        
        logger.info(f"üßÆ Calculating memory layout for {available_memory / 1024**3:.1f}GB")
        
        # Estimate memory per scenario (conservative)
        # Each scenario needs:
        # - 52 cards (deck) = 52 * 4 bytes = 208 bytes
        # - 6 player hands (12 cards) = 12 * 4 bytes = 48 bytes  
        # - Community cards (5 max) = 5 * 4 bytes = 20 bytes
        # - Equity results = 6 players * 4 bytes = 24 bytes
        # - Regret tables = 100 nodes * 4 actions * 4 bytes = 1600 bytes
        # - Strategy tables = 100 nodes * 4 actions * 4 bytes = 1600 bytes
        # Total per scenario ‚âà 3500 bytes + overhead
        
        bytes_per_scenario = 5000  # Conservative estimate with overhead
        
        # Calculate maximum batch size
        max_batch_size = int(available_memory / bytes_per_scenario)
        
        # Use powers of 2 for better GPU utilization
        optimal_batch_size = 1
        while optimal_batch_size * 2 <= max_batch_size:
            optimal_batch_size *= 2
            
        # Ensure we don't exceed reasonable limits
        self.max_batch_size = min(optimal_batch_size, 50000)
        self.equity_batch_size = min(10000, self.max_batch_size // 2)
        
        logger.info(f"üìè Memory layout calculated:")
        logger.info(f"   - Max batch size: {self.max_batch_size:,}")
        logger.info(f"   - Equity batch size: {self.equity_batch_size:,}")
        logger.info(f"   - Estimated memory per batch: {(self.max_batch_size * bytes_per_scenario) / 1024**3:.2f}GB")
        
    def _pre_allocate_gpu_memory(self):
        """Pre-allocate GPU memory pools for maximum efficiency."""
        
        logger.info(f"üîÑ Pre-allocating GPU memory pools...")
        
        try:
            # Pre-allocate scenario data arrays
            self.gpu_decks = cp.zeros((self.max_batch_size, 52), dtype=cp.int32)
            self.gpu_hands = cp.zeros((self.max_batch_size, self.num_players, 2), dtype=cp.int32)
            self.gpu_community = cp.zeros((self.max_batch_size, 5), dtype=cp.int32)
            self.gpu_pots = cp.zeros(self.max_batch_size, dtype=cp.float32)
            self.gpu_bets = cp.zeros((self.max_batch_size, self.num_players), dtype=cp.float32)
            
            # Pre-allocate equity calculation arrays
            self.gpu_equity_results = cp.zeros((self.max_batch_size, self.num_players), dtype=cp.float32)
            self.gpu_win_counts = cp.zeros((self.max_batch_size, self.num_players), dtype=cp.int32)
            
            # Pre-allocate CFR data structures  
            max_nodes = 1000
            max_actions = 4
            self.gpu_regrets = cp.zeros((self.max_batch_size, max_nodes, max_actions), dtype=cp.float32)
            self.gpu_strategies = cp.zeros((self.max_batch_size, max_nodes, max_actions), dtype=cp.float32)
            self.gpu_utilities = cp.zeros((self.max_batch_size, self.num_players), dtype=cp.float32)
            
            # Pre-allocate random states for reproducible simulations
            self.gpu_random_states = [cp.random.RandomState(seed=i) for i in range(100)]
            
            # Force GPU memory allocation
            cp.cuda.Stream.null.synchronize()
            
            # Check memory usage
            mempool = cp.get_default_memory_pool()
            memory_used = mempool.used_bytes()
            memory_total = mempool.total_bytes()
            
            logger.info(f"‚úÖ GPU memory pre-allocated:")
            logger.info(f"   - Used: {memory_used / 1024**3:.2f}GB")
            logger.info(f"   - Pool total: {memory_total / 1024**3:.2f}GB") 
            
            self.training_stats['memory_peak_usage'] = memory_used
            
        except Exception as e:
            logger.error(f"‚ùå Failed to pre-allocate GPU memory: {e}")
            raise
            
    def generate_batch_scenarios(self, batch_size):
        """Generate a batch of poker scenarios with GPU-optimized data structures."""
        
        current_batch_size = min(batch_size, self.max_batch_size)
        
        # Generate scenarios on CPU first, then transfer to GPU
        scenarios = []
        
        for i in range(current_batch_size):
            # Create shuffled deck
            deck_indices = list(range(52))
            np.random.shuffle(deck_indices)
            
            # Deal hands (2 cards per player)
            player_hands = []
            cards_dealt = 0
            for player in range(self.num_players):
                hand_indices = deck_indices[cards_dealt:cards_dealt + 2]
                player_hands.append(hand_indices)
                cards_dealt += 2
            
            # Community cards (up to 5)
            community_indices = deck_indices[cards_dealt:cards_dealt + 5]
            
            # Game state
            scenario = {
                'deck_indices': deck_indices,
                'hand_indices': player_hands,
                'community_indices': community_indices,
                'pot': 3.0,  # SB + BB
                'active_players': list(range(self.num_players))
            }
            scenarios.append(scenario)
            
        return scenarios
    
    def process_batch_gpu_massive(self, scenarios):
        """Process scenarios with maximum GPU utilization."""
        
        batch_size = len(scenarios)
        
        # Convert to card strings for equity calculation
        all_hands = []
        for scenario in scenarios:
            for hand_indices in scenario['hand_indices']:
                # Convert indices to card strings (Unicode format for GPU)
                hand_cards = [self.gpu_equity_calculator.all_cards[idx] for idx in hand_indices]
                all_hands.append(hand_cards)
        
        # Community cards (use first 3 for flop)
        community_cards = [self.gpu_equity_calculator.all_cards[scenarios[0]['community_indices'][i]] for i in range(3)]
        
        # Massive GPU equity calculation
        logger.info(f"üöÄ Processing {len(all_hands)} hands with GPU equity calculator...")
        
        equity_start = time.time()
        equities, mean_equity, std_equity = self.gpu_equity_calculator.calculate_equity_batch_gpu(
            all_hands,
            community_cards,
            num_simulations=self.equity_batch_size,
            num_opponents=2
        )
        equity_time = time.time() - equity_start
        
        logger.info(f"‚ö° GPU equity calculation: {equity_time:.3f}s for {len(all_hands)} hands")
        logger.info(f"üìä Equity stats - Mean: {mean_equity:.3f}, Std: {std_equity:.3f}")
        
        # Simulate CFR computations (simplified for maximum throughput)
        cfr_results = []
        for i, scenario in enumerate(scenarios):
            # Simplified CFR result based on equity
            player_equities = equities[i * self.num_players:(i + 1) * self.num_players]
            
            result = {
                'iteration': i,
                'equities': player_equities,
                'expected_value': np.mean(player_equities),
                'regrets_updated': True,
                'strategies_updated': True
            }
            cfr_results.append(result)
        
        self.training_stats['total_simulations'] += batch_size * self.equity_batch_size
        
        return cfr_results
    
    def train_memory_optimized(self, total_iterations=100000, adaptive_batching=True):
        """Train with maximum memory utilization and adaptive batch sizing."""
        
        logger.info(f"üöÄ MEMORY-OPTIMIZED GPU CFR TRAINING STARTED")
        logger.info(f"   Target iterations: {total_iterations:,}")
        logger.info(f"   Max batch size: {self.max_batch_size:,}")
        logger.info(f"   Adaptive batching: {adaptive_batching}")
        
        self.training_stats['start_time'] = time.time()
        
        # Start with maximum batch size
        current_batch_size = self.max_batch_size
        completed_iterations = 0
        
        # Performance tracking
        throughput_window = []
        last_log_time = time.time()
        
        while completed_iterations < total_iterations:
            batch_start_time = time.time()
            
            # Calculate remaining iterations
            remaining = total_iterations - completed_iterations
            actual_batch_size = min(current_batch_size, remaining)
            
            logger.info(f"üî• Batch: {actual_batch_size:,} iterations ({completed_iterations:,}/{total_iterations:,} - {100*completed_iterations/total_iterations:.1f}%)")
            
            try:
                # Generate scenarios
                scenario_start = time.time()
                scenarios = self.generate_batch_scenarios(actual_batch_size)
                scenario_time = time.time() - scenario_start
                
                # Process with GPU
                process_start = time.time()
                results = self.process_batch_gpu_massive(scenarios)
                process_time = time.time() - process_start
                
                batch_time = time.time() - batch_start_time
                throughput = actual_batch_size / batch_time
                
                # Update statistics
                self.training_stats['iterations_completed'] += actual_batch_size
                self.training_stats['batch_times'].append(batch_time)
                self.training_stats['throughput_history'].append(throughput)
                throughput_window.append(throughput)
                
                # Keep only recent throughput measurements
                if len(throughput_window) > 10:
                    throughput_window.pop(0)
                
                # Adaptive batch sizing based on performance
                if adaptive_batching and len(throughput_window) >= 3:
                    avg_throughput = np.mean(throughput_window)
                    if avg_throughput > 1000 and current_batch_size < self.max_batch_size:
                        current_batch_size = min(current_batch_size * 2, self.max_batch_size)
                        logger.info(f"üìà Increasing batch size to {current_batch_size:,}")
                    elif avg_throughput < 100 and current_batch_size > 100:
                        current_batch_size = max(current_batch_size // 2, 100)
                        logger.info(f"üìâ Decreasing batch size to {current_batch_size:,}")
                
                completed_iterations += actual_batch_size
                
                # Detailed logging every 30 seconds
                current_time = time.time()
                if current_time - last_log_time >= 30:
                    self._log_detailed_progress(completed_iterations, total_iterations, throughput_window)
                    last_log_time = current_time
                
                # Memory monitoring
                if completed_iterations % 5000 == 0:
                    self._log_memory_usage()
                
            except Exception as e:
                logger.error(f"‚ùå Batch processing failed: {e}")
                # Try smaller batch size
                current_batch_size = max(current_batch_size // 2, 100)
                logger.info(f"üîÑ Reducing batch size to {current_batch_size:,} and retrying")
                continue
        
        # Training completed
        total_time = time.time() - self.training_stats['start_time']
        self._log_final_statistics(total_time)
        
    def _log_detailed_progress(self, completed, total, throughput_window):
        """Log detailed training progress."""
        
        progress_pct = 100 * completed / total
        avg_throughput = np.mean(throughput_window) if throughput_window else 0
        
        elapsed = time.time() - self.training_stats['start_time'] 
        eta_seconds = (total - completed) / avg_throughput if avg_throughput > 0 else 0
        eta_minutes = eta_seconds / 60
        
        logger.info(f"üìä PROGRESS UPDATE:")
        logger.info(f"   ‚úÖ Completed: {completed:,} / {total:,} ({progress_pct:.1f}%)")
        logger.info(f"   ‚ö° Throughput: {avg_throughput:.1f} iterations/sec")
        logger.info(f"   ‚è±Ô∏è Elapsed: {elapsed/60:.1f} minutes")
        logger.info(f"   üéØ ETA: {eta_minutes:.1f} minutes")
        logger.info(f"   üî¢ Total simulations: {self.training_stats['total_simulations']:,}")
        
    def _log_memory_usage(self):
        """Log current GPU memory usage."""
        
        mempool = cp.get_default_memory_pool()
        used_bytes = mempool.used_bytes()
        total_bytes = mempool.total_bytes()
        
        logger.info(f"üíæ Memory: {used_bytes/1024**3:.2f}GB used / {total_bytes/1024**3:.2f}GB allocated")
        
        self.training_stats['memory_peak_usage'] = max(self.training_stats['memory_peak_usage'], used_bytes)
        
    def _log_final_statistics(self, total_time):
        """Log comprehensive final training statistics."""
        
        avg_batch_time = np.mean(self.training_stats['batch_times']) if self.training_stats['batch_times'] else 0
        avg_throughput = np.mean(self.training_stats['throughput_history']) if self.training_stats['throughput_history'] else 0
        peak_memory_gb = self.training_stats['memory_peak_usage'] / 1024**3
        
        logger.info(f"üèÅ MEMORY-OPTIMIZED TRAINING COMPLETED!")
        logger.info(f"üìà FINAL STATISTICS:")
        logger.info(f"   ‚è±Ô∏è Total time: {total_time/60:.1f} minutes")
        logger.info(f"   ‚úÖ Iterations completed: {self.training_stats['iterations_completed']:,}")
        logger.info(f"   üî¢ Total simulations: {self.training_stats['total_simulations']:,}")
        logger.info(f"   ‚ö° Average throughput: {avg_throughput:.1f} iterations/sec")
        logger.info(f"   üì¶ Average batch time: {avg_batch_time:.3f} seconds")
        logger.info(f"   üíæ Peak memory usage: {peak_memory_gb:.2f}GB")
        logger.info(f"   üöÄ Simulation rate: {self.training_stats['total_simulations']/total_time:.0f} sims/sec")
        
        # Save statistics to file
        stats_file = f"memory_optimized_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        stats_data = {
            'total_time_minutes': total_time / 60,
            'iterations_completed': self.training_stats['iterations_completed'],
            'total_simulations': self.training_stats['total_simulations'],
            'average_throughput': avg_throughput,
            'peak_memory_gb': peak_memory_gb,
            'simulation_rate': self.training_stats['total_simulations'] / total_time,
            'batch_size_used': self.max_batch_size
        }
        
        with open(stats_file, 'w') as f:
            json.dump(stats_data, f, indent=2)
        
        logger.info(f"üìä Statistics saved to {stats_file}")

def main():
    """Run memory-optimized GPU CFR training."""
    
    logger.info("üöÄ MEMORY-OPTIMIZED GPU CFR TRAINING")
    logger.info("=====================================")
    
    # Check GPU memory
    if not GPU_AVAILABLE:
        logger.error("‚ùå GPU not available!")
        return
    
    try:
        # Initialize trainer with maximum memory utilization
        trainer = MemoryOptimizedGPUTrainer(num_players=6, memory_fraction=0.85)
        
        # Run training with maximum iterations
        trainer.train_memory_optimized(
            total_iterations=200000,  # Very large training run
            adaptive_batching=True
        )
        
    except Exception as e:
        logger.error(f"‚ùå Training failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
